{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Bart\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    corpus = 'fm_fc_ms_ff' #<-- Scope\n",
    "    #data_path = Path('../semeru-datasets/athena_test/' + corpus + '/')\n",
    "    data_path_raw = Path('../semeru-datasets/athena_test/'+ corpus + '/raw/')\n",
    "    tokenizer_path = Path('../scripts/tokenizer/')\n",
    "    return {\n",
    "        'bpe_path' : tokenizer_path / 'universal_tokenizer/roberta_aug_spaces',\n",
    "        'eval_raw': [data_path_raw / 'eval/input.methods.txt',\n",
    "                        data_path_raw / 'eval/output.tests.txt'],\n",
    "        'test_raw': [data_path_raw / 'test/input.methods.txt', \n",
    "                        data_path_raw / 'test/output.tests.txt'],\n",
    "        'train_raw': [data_path_raw / 'train/input.methods.txt', \n",
    "                        data_path_raw / 'train/output.tests.txt'],\n",
    "        'data_labels' : ['test_raw'],#['eval_raw','test_raw','train_raw'], <----- Just Test\n",
    "        #'output_pandas' : data_path / 'pandas/',\n",
    "        'out_processed' : '/workspaces/code-rationales/data/athena-test-out/out_processed/',\n",
    "        'model_name_or_path' : '/workspaces/code-rationales/data/bart-fairseq/checkpoint_dir_athena_ms/models/', #Model Path\n",
    "        'checkpoint_file': 'checkpoint_best.pt', #Model\n",
    "        #'data_preprocessed':'/home/davidna/data/dummy/sequential-rationales/fairseq/fairseq/data-bin/bins/',\n",
    "        'output_results' : '/workspaces/code-rationales/data/athena-test-out/icse_results/' \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint_best.pt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = param_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../scripts/tokenizer/universal_tokenizer/roberta_aug_spaces')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['bpe_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(bpe_path):\n",
    "    return ByteLevelBPETokenizer(str(bpe_path)+'-vocab.json',str(bpe_path)+'-merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(params['bpe_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../semeru-datasets/athena_test/fm_fc_ms_ff/raw/test/input.methods.txt'),\n",
       " PosixPath('../semeru-datasets/athena_test/fm_fc_ms_ff/raw/test/output.tests.txt')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['test_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def method_size_vector( method_vector ):\n",
    "    '''Return the size of the tokens for a give method based on id\n",
    "        Assuming that method_vector is an array of tokens\n",
    "    '''\n",
    "    input_ids = [ len(mtd) for mtd in method_vector ]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input file names\n",
    "inputs = \"input.methods.txt\"\n",
    "outputs = \"output.tests.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acccctualllyy, we don't need to normalize anything! The BPE tokenizer will take care of this.\n",
    "#Consider using a linter to standardize the java code\n",
    "def normalize_method(method):\n",
    "    return method\n",
    "    #return method.replace(' . ','.').replace(' ;',';').replace(' ,',',').replace(' ( ','(').replace(' )',')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode_and_write(src_file,dest_file,tokenizer):\n",
    "    with open(src_file,'r') as r, open(dest_file,'w') as w:\n",
    "        lines = r.readlines()\n",
    "        lines = [normalize_method(line.rstrip()) for line in lines]\n",
    "        bpe_encodings = [' '.join(enc.tokens) for enc in tokenizer.encode_batch(lines)]\n",
    "        for line in bpe_encodings:\n",
    "            w.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode_folder(corpus_folder, processed_folder):\n",
    "    for split in ['train','eval','test']:\n",
    "        for basename in [inputs, outputs]:\n",
    "            src_file = Path(corpus_folder)/Path(split)/Path(basename)\n",
    "            print('encoding \\t'+str(src_file))\n",
    "            dest_file = Path(processed_folder)/Path(split+'.'+basename[:-4])\n",
    "            bpe_encode_and_write(src_file,dest_file,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
