{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Athena Training Notebook (from Scratch)\n",
    "\n",
    ">\n",
    "> Excercise to replicate Athena Training by @davidN\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    corpus = 'fm_fc_ms_ff' #<-- Scope\n",
    "    data_path = Path('../athena-datasets/' + corpus + '/')\n",
    "    data_path_raw = Path('../athena-datasets/' + corpus + '/raw/')\n",
    "    tokenizer_path = Path('../tokenizer/')\n",
    "    return {\n",
    "        'bpe_path' : tokenizer_path / 'universal_tokenizer/universal_tokenizer/roberta_aug_spaces',\n",
    "        'eval_raw': [data_path_raw / 'eval/input.methods.txt',\n",
    "                        data_path_raw / 'eval/output.tests.txt'],\n",
    "        'test_raw': [data_path_raw / 'test/input.methods.txt', \n",
    "                        data_path_raw / 'test/output.tests.txt'],\n",
    "        'train_raw': [data_path_raw / 'train/input.methods.txt', \n",
    "                        data_path_raw / 'train/output.tests.txt'],\n",
    "        'data_labels' : ['eval_raw','test_raw','train_raw'],\n",
    "        'output_pandas' : data_path / 'pandas/',\n",
    "        'out_processed' : '/datasets/out_processed/',\n",
    "        'in_model' : '~/data/dummy/models/checkpoint_best_mod.pt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = param_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input file names\n",
    "inputs = \"input.methods.txt\"\n",
    "outputs = \"output.tests.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acccctualllyy, we don't need to normalize anything! The BPE tokenizer will take care of this.\n",
    "#Consider using a linter to standardize the java code\n",
    "def normalize_method(method):\n",
    "    return method\n",
    "    #return method.replace(' . ','.').replace(' ;',';').replace(' ,',',').replace(' ( ','(').replace(' )',')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(bpe_path):\n",
    "    return ByteLevelBPETokenizer(str(bpe_path)+'-vocab.json',str(bpe_path)+'-merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(params['bpe_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode_and_write(src_file,dest_file,tokenizer):\n",
    "    with open(src_file,'r') as r, open(dest_file,'w') as w:\n",
    "        lines = r.readlines()\n",
    "        lines = [normalize_method(line.rstrip()) for line in lines]\n",
    "        bpe_encodings = [' '.join(enc.tokens) for enc in tokenizer.encode_batch(lines)]\n",
    "        for line in bpe_encodings:\n",
    "            w.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode_folder(corpus_folder, processed_folder):\n",
    "    for split in ['train','eval','test']:\n",
    "        for basename in [inputs, outputs]:\n",
    "            src_file = Path(corpus_folder)/Path(split)/Path(basename)\n",
    "            print('encoding \\t'+str(src_file))\n",
    "            dest_file = Path(processed_folder)/Path(split+'.'+basename[:-4])\n",
    "            bpe_encode_and_write(src_file,dest_file,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairseq Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessing_command(processed_root, src_dict):\n",
    "    '''\n",
    "    Writes the command for preprocessing in fairseq @davidN\n",
    "    '''\n",
    "    #src_dict = \"/tufanodata/work/unit-test-gen/code/universal_tokenizer/universal_tokenizer/roberta_aug_spaces_dict.txt\" \n",
    "    dest_dir = processed_root / 'bins/'\n",
    "    src_dir_pref = processed_root\n",
    "\n",
    "    src_ext = inputs\n",
    "    tgt_ext = outputs\n",
    "\n",
    "    source_lang = \"input.methods\"\n",
    "    target_lang = \"output.tests\"\n",
    "\n",
    "    trainpref = src_dir_pref / 'train'\n",
    "    validpref = src_dir_pref / 'eval'\n",
    "    testpref = src_dir_pref / 'test'\n",
    "\n",
    "    command =\"\"\"\n",
    "            fairseq-preprocess \\\n",
    "            --source-lang \"\"\" +  source_lang + \"\"\" \\\n",
    "            --target-lang \"\"\" + target_lang + \"\"\" \\\n",
    "            --trainpref \"\"\" + str(trainpref) + \"\"\" \\\n",
    "            --validpref \"\"\" + str(validpref) + \"\"\" \\\n",
    "            --testpref \"\"\" + str(testpref) + \"\"\" \\\n",
    "            --destdir \"\"\" + str(dest_dir) + \"\"\" \\\n",
    "            --workers 24 \\\n",
    "            --srcdict \"\"\" + str(src_dict) + \"\"\" \\\n",
    "            --joined-dictionary \\\n",
    "            \"\"\"\n",
    "    print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            fairseq-preprocess             --source-lang input.methods             --target-lang output.tests             --trainpref ../tokenizer/universal_tokenizer/universal_tokenizer/roberta_aug_spaces/train             --validpref ../tokenizer/universal_tokenizer/universal_tokenizer/roberta_aug_spaces/eval             --testpref ../tokenizer/universal_tokenizer/universal_tokenizer/roberta_aug_spaces/test             --destdir ../tokenizer/universal_tokenizer/universal_tokenizer/roberta_aug_spaces/bins             --workers 24             --srcdict /datasets/out_processed/             --joined-dictionary             \n"
     ]
    }
   ],
   "source": [
    "#Processing a Folder\n",
    "#processed_folder = corpus_root + \"fm_fc_ms_ff\" + \"/processed/\"\n",
    "build_preprocessing_command(processed_root = params['bpe_path'], src_dict = params['out_processed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export TOTAL_NUM_UPDATES=100000\n",
    "! export WARMUP_UPDATES=10000\n",
    "! export LR=4.2e-05\n",
    "! export UPDATE_FREQ=8\n",
    "! export DIR=/tufanodata/work/unit-test-gen-context/results-models/fm_fc_ms_ff\n",
    "! export MAX_TOKENS=1024\n",
    "! export PRETRAINED=/tufanodata/work/unit-test-gen-context/models/bart-english+java/java_finetune_from_english_filtered.pt\n",
    "! export DATA_DIR=/tufanodata/work/unit-test-gen-context/data/corpus/fm_fc_ms_ff/processed/bins\n",
    "! export SRC_LANG=input.methods\n",
    "! export TRG_LANG=output.tests"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27c2fcb21fdb148cd37ecbed2ef65b6b1f3a0948b222c0bcf7dcf1d6a4c7a458"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('shapley-01': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
