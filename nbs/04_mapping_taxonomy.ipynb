{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "> Labels a given token following a tailored taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-08-29 16:09:22.383010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 16:09:22.593645: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.loader import download_grammars\n",
    "from tree_sitter import Language, Parser\n",
    "import code_rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    return {\n",
    "        #'dataset' : 'code_completion_random_cut_5k_30_512_tokens',\n",
    "        'dataset' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_signature_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_5k_30_150_tokens',\n",
    "        'rational_results': '/workspaces/code-rationales/data/rationales/gpt',\n",
    "        'global_ast_results': '/workspaces/code-rationales/data/global_ast_results/gpt',\n",
    "        'global_taxonomy_results': '/workspaces/code-rationales/data/global_taxonomy_results/gpt',\n",
    "        'delimiter_sequence': 'and code starts with',\n",
    "        'num_samples' : 100, \n",
    "        'size_samples' : 146,\n",
    "        'num_experiments': 1, \n",
    "        'bootstrapping' : 500\n",
    "    }\n",
    "params = param_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programming Language Taxonomy\n",
    "def pl_taxonomy_python() -> dict:\n",
    "    return {\n",
    "  \"punctuation\": ['{', '}', '[', ']', '(', ')','\\\"', ',', '.', '...', ';', ':'], \n",
    "  \"exceptions\": ['raise_statement','catch', 'try', 'finally', 'throw', 'throws', 'except'],\n",
    "  \"oop\": ['def','class','instanceof','interface','private','protected','public','abstract','extends','package','this','implements','import','new','super'],\n",
    "  \"asserts\": ['assert'],\n",
    "  \"types\": ['tuple','set','list','pair','subscript','type','none','dictionary','integer','native','static','synchronized','transient','volatile','void','final','enum','byte','char','float','boolean','double','int','long','short','strictfp'],\n",
    "  \"conditionals\": ['else', 'if', 'switch', 'case', 'default'],\n",
    "  \"loops\": ['break', 'do', 'for', 'while', 'continue'],\n",
    "  \"operators\": ['as','yield','is','@','in','and','or','not','**','slice','%','+','<','>','=','+','-','*','/','%','++','--','!','==','!=','>=','<=','&&','||','?',':','~','<<','>>','>>>','&','^','|','//'],\n",
    "  \"indentation\": ['\\n','\\t'],\n",
    "  \"bool\": ['true', 'false'], \n",
    "  \"functional\":['lambda','lambda_parameters'],\n",
    "  \"with\" : ['with','with_item','with_statement','with_clause'], \n",
    "  \"return\" :['return'],\n",
    "  \"structural\" : ['attribute','module', 'argument_list','parenthesized_expression','pattern_list','class_definition','function_definition','block'],\n",
    "  \"statements\" : ['return_statement','break_statement','assignment','while_statement','expression_statement','assert_statement'],\n",
    "  \"expression\": ['call','exec','async','ellipsis','unary_operator','binary_operator','as_pattern_target','boolean_operator','as_pattern','comparison_operator','conditional_expression','named_expression','not_operator','primary_expression','as_pattern'],\n",
    "  \"errors\": [\"ERROR\"],\n",
    "  \"identifier\":[\"identifier\"],  \n",
    "  \"comment\":[\"comment\"],\n",
    "  \"string\": ['string','interpolation','string_content','string_end','string_start','escape_sequence'], \n",
    "  \"unknown\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_pos_taxonomy() -> dict: return {\n",
    "    \"nl_verb\" : ['VBN', 'VBG', 'VBZ', 'VBP', 'VBD', 'VB'],\n",
    "    \"nl_noun\" : ['NN', 'NNPS', 'NNS', 'NNP'],\n",
    "    \"nl_pronoun\" : ['WP', 'PRP', 'PRP$', 'WP','WP$'], \n",
    "    \"nl_adverb\" : ['RBS','RBR', 'RB', 'WRB'], \n",
    "    \"nl_adjetive\" : ['JJR', 'JJS', 'JJ'], \n",
    "    \"nl_determier\" : ['DT','WDT','PDT'], \n",
    "    \"nl_preposition\" : ['IN', 'TO'],\n",
    "    \"nl_particle\" : ['RP'],\n",
    "    \"nl_modal\" : ['MD'],\n",
    "    \"nl_conjunction\" : ['CC'],\n",
    "    \"nl_cardinal\" : ['CD'],\n",
    "    \"nl_list\": ['LS'],\n",
    "    \"nl_other\" : ['FW', 'EX', 'SYM' , 'UH', 'POS', \"''\", '--',':', '(', ')', '.', ',', '``', '$']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/code_rationales/grammars\n"
     ]
    }
   ],
   "source": [
    "languages=['python', 'java']\n",
    "download_grammars(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_node_types(\n",
    "    nested_node_types: dict  # node_types from tree-sitter\n",
    ") -> list: # list of node types\n",
    "    def iterate_and_unroll_dict(nested_node_types: dict, all_node_types: set):\n",
    "        for key, value in nested_node_types.items():\n",
    "            if key == 'type' and type(value) == str:\n",
    "                all_node_types.add(value)\n",
    "            if type(value) == dict:\n",
    "                iterate_and_unroll_dict(value, all_node_types)\n",
    "            if type(value) == list:\n",
    "                for element in value:\n",
    "                    iterate_and_unroll_dict(element, all_node_types) \n",
    "    all_node_types = set()\n",
    "    for dictionary in nested_node_types:\n",
    "        iterate_and_unroll_dict(dictionary, all_node_types)\n",
    "    all_node_types.add('ERROR')\n",
    "    return list(all_node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(lang: str):\n",
    "    # Grab the node types from the tree-sitter language\n",
    "    language = Language(f\"{code_rationales.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "    node_path = f\"{code_rationales.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "    with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "    node_types = unroll_node_types(node_types)\n",
    "    # Create a parser for the language\n",
    "    parser = Parser()\n",
    "    parser.set_language(language)\n",
    "    return parser, node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(\n",
    "    node,       # tree-sitter node\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    results = []\n",
    "    def traverse_tree(node, results):\n",
    "        if node.type == 'string':\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_tree(n, results)\n",
    "        if not node.children:\n",
    "            results.append(node)\n",
    "    traverse_tree(node, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_offset(\n",
    "    point,              #point to convert\n",
    "    lines: list         #list of lines in the source code\n",
    "    ):\n",
    "        \"\"\"Convert the point to an offset\"\"\"\n",
    "        row, column = point\n",
    "        chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "        chars_in_columns = len(lines[row][:column])\n",
    "        offset = chars_in_rows + chars_in_columns\n",
    "        return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_span(node, lines):\n",
    "    \"\"\"Get the span position of the node in the code string\"\"\"\n",
    "    start_span = convert_to_offset(node.start_point, lines)\n",
    "    end_span = convert_to_offset(node.end_point, lines)\n",
    "    return start_span, end_span\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_span_in_node_span(tok_span, node_span):\n",
    "    if (node_span[0] <= tok_span[0] and tok_span[0] < node_span[1]) or (node_span[0] < tok_span[1] and tok_span[1] <= node_span[1]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    node_spans = [get_node_span(node, lines) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if is_token_span_in_node_span(tok_span, span):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_nodes(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    node,            # tree-sitter node\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> list: \n",
    "    \"\"\"Get all AST types for the given token span\"\"\"\n",
    "    results = []\n",
    "    def traverse_and_get_types(tok_span, node, lines, results) -> None:\n",
    "        node_span = get_node_span(node, lines)\n",
    "        if (node_span[0] <= tok_span[0] and tok_span[0] < node_span[1]) or (node_span[0] < tok_span[1] and tok_span[1] <= node_span[1]):\n",
    "            results.append(node)\n",
    "        for n in node.children:\n",
    "            traverse_and_get_types(tok_span, n, lines, results)\n",
    "    traverse_and_get_types(tok_span, node, lines, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_by_type(\n",
    "    node, \n",
    "    node_types: list\n",
    ") -> list :\n",
    "    def traverse_and_search(node, node_types, results):\n",
    "        if node.type in node_types:\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_and_search(n, node_types ,results)\n",
    "    results = []\n",
    "    traverse_and_search(node, node_types, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(global_results):\n",
    "    def clean_dictonary(result_dict):\n",
    "        clean_dict = result_dict.copy()\n",
    "        for key, value in result_dict.items():\n",
    "            if not value:\n",
    "                clean_dict.pop(key)\n",
    "        return clean_dict\n",
    "    for key, value in global_results.items():\n",
    "        global_results[key] = clean_dictonary(value)\n",
    "    return clean_dictonary(global_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_category_by_token(taxonomy_dict: dict, token_type: str):\n",
    "    for key, value in taxonomy_dict.items():\n",
    "        if token_type in value:\n",
    "            return key\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_taxonomy(taxonomy_dict: dict, result_dict: dict):\n",
    "    result_dict = result_dict.copy()\n",
    "    mappings = {token: {category : [] for category in taxonomy_dict.keys()} for token in taxonomy_dict.keys()}\n",
    "    for target_token, value in result_dict.items():\n",
    "        for source_token, probs in value.items():\n",
    "            mappings[search_category_by_token(taxonomy_dict, target_token)][search_category_by_token(taxonomy_dict, source_token)]+=probs\n",
    "    return clean_results(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_global_results_to_taxonomy(taxonomy_dict:dict, global_results: dict):\n",
    "    return dict(zip(global_results.keys(), map(lambda aggegrations: map_to_taxonomy(taxonomy_dict, aggegrations), global_results.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rationals Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_right_span = lambda start_idx, end_idx, df : len(''.join(map(str, df.loc[start_idx:end_idx, 'goal_token'].tolist())))\n",
    "calculate_span = lambda right_span, token : (right_span-len(str(token)), right_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_auxiliary_columns_to_experiment_result(df, delimiter_sequence: str):\n",
    "    initial_token = eval(df['typesets_tgt'][0])[0][0]\n",
    "    ### TOKEN TYPE COLUMN\n",
    "    token_type_column = ['src'] * len(df)\n",
    "    sequence = initial_token\n",
    "    for idx, goal_token in enumerate(df['goal_token']):\n",
    "        if delimiter_sequence not in sequence:\n",
    "            token_type_column[idx] = 'nl'\n",
    "            sequence+=goal_token\n",
    "    df['token_type'] = token_type_column\n",
    "    ### TOKEN SPAN COLUMN - CHECK FOR DATASETS\n",
    "    src_initial_token_idx = df[df['token_type']== 'src'].first_valid_index()\n",
    "    df['span'] = [None] * len(df[:src_initial_token_idx]) + [calculate_span(calculate_right_span(src_initial_token_idx, index, df), token) for index, token in df[src_initial_token_idx:]['goal_token'].items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nl_tags_in_experiment_result(df, nl_ast_types, nl_pos_types, parser):\n",
    "    initial_token = eval(df['typesets_tgt'][0])[0][0]\n",
    "    ##### POS TAGS FOR NL PART\n",
    "    target_nl = initial_token + ''.join(df[df['token_type'] == 'nl']['goal_token'].map(lambda value: str(value)))\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(target_nl))\n",
    "    for idx in range(df[df['token_type']== 'src'].first_valid_index()):\n",
    "        nl_tags = list(map(lambda tag: tag[1] if tag[1] in nl_pos_types else None, filter(lambda tag: tag[0] in str(df['goal_token'][idx]), pos_tags)))\n",
    "        if nl_tags: df.at[idx, 'tags'] = df['tags'][idx] + [nl_tags[-1]]\n",
    "    ##### POS TAGS FOR CODE PART\n",
    "    target_code = ''.join(df[df['token_type'] == 'src']['goal_token'].map(lambda value: str(value)))\n",
    "    nl_target_nodes = get_nodes_by_type(parser.parse(bytes(target_code, 'utf8')).root_node, nl_ast_types)\n",
    "    for token_idx in range(df[df['token_type'] == 'src'].first_valid_index(), len(df['span'])):\n",
    "                for nl_target_node in nl_target_nodes:\n",
    "                    if is_token_span_in_node_span(df['span'][token_idx], get_node_span(nl_target_node, target_code.split(\"\\n\"))) and \\\n",
    "                            str(df['goal_token'][token_idx]) in nl_target_node.text.decode('utf-8'):\n",
    "                            tagged_token_list = list(filter(lambda tagged_token: tagged_token[0] in str(df['goal_token'][token_idx]), \\\n",
    "                                                        nltk.pos_tag( nltk.word_tokenize(nl_target_node.text.decode('utf-8')))))\n",
    "                            if len(tagged_token_list)>0 and tagged_token_list[0][1] in nl_pos_types and tagged_token_list[0][1] not in df['tags'][token_idx]: df['tags'][token_idx].append(tagged_token_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_ast_tags_in_experiment_result(df, parser):\n",
    "    initial_token = eval(df['typesets_tgt'][0])[0][0] if df[df['token_type'] == 'src'].first_valid_index() == 0 else ''\n",
    "    target_code = initial_token + ''.join(df[df['token_type'] == 'src']['goal_token'].map(lambda value: str(value)))\n",
    "    src_initial_token_idx = df[df['token_type']== 'src'].first_valid_index()\n",
    "    target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n",
    "    for token_idx in range(src_initial_token_idx, len(df)):\n",
    "        df.at[token_idx, 'tags'] = df['tags'][token_idx] + list(map(lambda node: node.type, get_token_nodes(df['span'][token_idx], target_ast, target_code.split(\"\\n\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_rationals(experiment_paths: list, nl_ast_types: list, nl_pos_types: list, delimiter_sequence: str, parser):\n",
    "    experiments = {}\n",
    "    for exp_idx, experiment_path in enumerate(experiment_paths):\n",
    "        experiment_results = []\n",
    "        df_experiment = pd.read_csv(experiment_path, index_col=0)\n",
    "        experiment_rational_results = [df_experiment[(df_experiment['from_seq_id'] == sample_idx) | \\\n",
    "                                                     (df_experiment['from_seq_id'] == str(sample_idx))].reset_index() \\\n",
    "                                                    for sample_idx in range(params['num_samples'])]\n",
    "        print('*'*10 +'Tagging rationals for exp: ' +str(exp_idx) + '*'*10)\n",
    "        for experiment_rational_result in experiment_rational_results:\n",
    "            add_auxiliary_columns_to_experiment_result(experiment_rational_result, delimiter_sequence)\n",
    "            experiment_rational_result['tags'] = [[]]*len(experiment_rational_result)\n",
    "            fill_nl_tags_in_experiment_result(experiment_rational_result, nl_ast_types, nl_pos_types, parser)\n",
    "            fill_ast_tags_in_experiment_result(experiment_rational_result, parser)\n",
    "            experiment_results.append(experiment_rational_result)\n",
    "        experiments[exp_idx] = experiment_results\n",
    "    return experiments\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationals Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rationals(global_tagged_results: dict, ast_node_types: list, nl_pos_types: list):\n",
    "    aggregation_results = {exp_id: None for exp_id in global_tagged_results.keys()}\n",
    "    for exp_idx, experiment_results in global_tagged_results.items():\n",
    "        experiment_aggregation_results = {node_type : {node_type : [] for node_type in ast_node_types + nl_pos_types} for node_type in ast_node_types + nl_pos_types}\n",
    "        print('*'*10 +'Aggregrating rationals for exp: ' +str(exp_idx) + '*'*10)\n",
    "        for result_idx, experiment_result in enumerate(experiment_results):\n",
    "            for target_idx in range(len(experiment_result)):\n",
    "                for rational_idx, rational_pos in enumerate(eval(experiment_result['rationale_pos_tgt'][target_idx])):\n",
    "                    try:\n",
    "                        [experiment_aggregation_results[target_tag][rational_tag].append(eval(experiment_result['rationale_prob_tgt'][target_idx])[rational_idx]) if target_tag and rational_tag else None \\\n",
    "                         for rational_tag in experiment_result['tags'][rational_pos] for target_tag in experiment_result['tags'][target_idx]]\n",
    "                    except Exception as e:\n",
    "                        print('An Error Occurred')\n",
    "            print('r-'+str(result_idx))\n",
    "        aggregation_results[exp_idx] = clean_results(experiment_aggregation_results)\n",
    "    return aggregation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping( np_data, np_func, size ):\n",
    "    \"\"\"Create a bootstrap sample given data and a function\n",
    "    For instance, a bootstrap sample of means, or mediands. \n",
    "    The bootstrap replicates are a long as the original size\n",
    "    we can choose any observation more than once (resampling with replacement:np.random.choice)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Cleaning NaNs\n",
    "    #np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n",
    "    \n",
    "    #The size of the bootstrap replicate is as big as size\n",
    "    #Creating the boostrap replicates as long as the orignal data size\n",
    "    #This strategy might work as imputation \n",
    "    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]\n",
    "    \n",
    "    #logging.info(\"Covariate: \" + cov) #Empirical Mean\n",
    "    #logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean))) #Empirical Mean\n",
    "    #logging.info(\"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) ) #Bootstrapped Mean\n",
    "    \n",
    "    return np.array( bootstrap_repl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_samples_global_results(global_results: dict, size: int):\n",
    "    for exp_id in global_results.keys():\n",
    "        experiment_result = global_results[exp_id]\n",
    "        for target_type, target_value in global_results[exp_id].items():\n",
    "            for source_type, source_value in target_value.items():\n",
    "                experiment_result[target_type][source_type] = bootstrapping(source_value, np.mean, size).tolist()\n",
    "        global_results[exp_id] = experiment_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve experiments\n",
    "get_experiment_path =  lambda samples, size, exp: params['rational_results'] + '/' + params['dataset'] + '/' + '[t_'+str(samples)+']_[max_tgt_'+str(size)+']_[exp:'+str(exp)+'].csv'\n",
    "experiment_paths = [get_experiment_path(params['num_samples'], params['size_samples'], exp) for exp in range(params['num_experiments'])][:1]\n",
    "### Define parser\n",
    "parser, node_types = create_parser('python')\n",
    "### Defines pos tags \n",
    "pos_types = list(nltk.data.load('help/tagsets/upenn_tagset.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Tagging rationals for exp: 0**********\n"
     ]
    }
   ],
   "source": [
    "###TAG EXPERIMENTS RESULTS - TAKES TIME\n",
    "nl_ast_types = ['comment','identifier','string']\n",
    "global_tagged_results = tag_rationals(experiment_paths, nl_ast_types, pos_types, params['delimiter_sequence'], parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Aggregrating rationals for exp: 0**********\n",
      "r-0\n",
      "r-1\n",
      "r-2\n",
      "r-3\n",
      "r-4\n",
      "r-5\n",
      "r-6\n",
      "r-7\n",
      "r-8\n",
      "r-9\n",
      "r-10\n",
      "r-11\n",
      "r-12\n",
      "r-13\n",
      "r-14\n",
      "r-15\n",
      "r-16\n",
      "r-17\n",
      "r-18\n",
      "r-19\n",
      "r-20\n",
      "r-21\n",
      "r-22\n",
      "r-23\n",
      "r-24\n",
      "r-25\n",
      "r-26\n",
      "r-27\n",
      "r-28\n",
      "r-29\n",
      "r-30\n",
      "r-31\n",
      "r-32\n",
      "r-33\n",
      "r-34\n",
      "r-35\n",
      "r-36\n",
      "r-37\n",
      "r-38\n",
      "r-39\n",
      "r-40\n",
      "r-41\n",
      "r-42\n",
      "r-43\n",
      "r-44\n",
      "r-45\n",
      "r-46\n",
      "r-47\n",
      "r-48\n",
      "r-49\n",
      "r-50\n",
      "r-51\n",
      "r-52\n",
      "r-53\n",
      "r-54\n",
      "r-55\n",
      "r-56\n",
      "r-57\n",
      "r-58\n",
      "r-59\n",
      "r-60\n",
      "r-61\n",
      "r-62\n",
      "r-63\n",
      "r-64\n",
      "r-65\n",
      "r-66\n",
      "r-67\n",
      "r-68\n",
      "r-69\n",
      "r-70\n",
      "r-71\n",
      "r-72\n",
      "r-73\n",
      "r-74\n",
      "r-75\n",
      "r-76\n",
      "r-77\n",
      "r-78\n",
      "r-79\n",
      "r-80\n",
      "r-81\n",
      "r-82\n",
      "r-83\n",
      "r-84\n",
      "r-85\n",
      "r-86\n",
      "r-87\n",
      "r-88\n",
      "r-89\n",
      "r-90\n",
      "r-91\n",
      "r-92\n",
      "r-93\n",
      "r-94\n",
      "r-95\n",
      "r-96\n",
      "r-97\n",
      "r-98\n",
      "r-99\n"
     ]
    }
   ],
   "source": [
    "###AGGREGATE RATIONALS - TAKES TIME\n",
    "global_aggregated_results = aggregate_rationals(global_tagged_results, node_types, pos_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GROUP AGGREGATES BY TAXONOMY\n",
    "taxonomy = {**pl_taxonomy_python(), **nl_pos_taxonomy()}\n",
    "global_taxonomy_results = map_global_results_to_taxonomy(taxonomy, global_aggregated_results.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOOTSTRAPPING - TAKES TIME\n",
    "bootstrap_samples_global_results(global_taxonomy_results, params['bootstrapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id, exp_aggregation in global_aggregated_results.items():\n",
    "    with open(params['global_ast_results'] + '/' + params['dataset'] +'_exp_' + str(exp_id) +'.txt', 'w') as file:\n",
    "        file.write(json.dumps(exp_aggregation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id, exp_aggregation in global_taxonomy_results.items():\n",
    "    with open(params['global_taxonomy_results'] + '/' + params['dataset'] +'_exp_' + str(exp_id) +'.txt', 'w') as file:\n",
    "        file.write(json.dumps(exp_aggregation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
